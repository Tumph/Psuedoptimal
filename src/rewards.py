"""Custom reward function for LLM-DSL training with GRPOTrainer."""

from dataclasses import dataclass, field
from typing import Callable, Optional

import torch
from transformers import PreTrainedModel, PreTrainedTokenizer

from .prompts import apply_chat_template, extract_python_code, format_generator_prompt
from .sandbox import SafeSandbox, prepare_test_code


@dataclass
class RewardConfig:
    """Configuration for reward computation."""

    pass_reward: float = 10.0
    fail_reward: float = -1.0
    length_penalty_coef: float = 0.05
    max_compression_ratio: float = 5.0
    max_dsl_tokens: int = 512
    sandbox_timeout: float = 5.0
    sandbox_memory_mb: int = 200
    generator_max_new_tokens: int = 512
    generator_temperature: float = 0.1


class DSLRewardFunction:
    """
    Custom reward function for emergent communication GRPO training.

    This class encapsulates the reward computation logic:
    1. Take compressed encodings from the Student model
    2. Expand them to Python using the frozen Generator model (NO task context!)
    3. Execute Python against MBPP tests in a sandbox
    4. Compute reward: (pass_reward × compression_ratio) if passed else fail_reward

    Critical: The Generator receives ONLY the encoding, not the original task.
    This forces the Student to encode all necessary information.
    """

    __name__ = "dsl_reward"  # Required by GRPOTrainer

    def __init__(
        self,
        generator_model: PreTrainedModel,
        generator_tokenizer: PreTrainedTokenizer,
        student_tokenizer: PreTrainedTokenizer,
        config: Optional[RewardConfig] = None,
    ):
        """
        Initialize the reward function.

        Args:
            generator_model: Frozen Generator model for DSL→Python expansion
            generator_tokenizer: Generator's tokenizer
            student_tokenizer: Student's tokenizer (for token counting)
            config: Reward configuration
        """
        self.generator_model = generator_model
        self.generator_tokenizer = generator_tokenizer
        self.student_tokenizer = student_tokenizer
        self.config = config or RewardConfig()

        # Initialize sandbox
        self.sandbox = SafeSandbox(
            timeout=self.config.sandbox_timeout,
            memory_limit_mb=self.config.sandbox_memory_mb,
        )

        # Ensure generator is frozen and in eval mode
        self.generator_model.eval()
        for param in self.generator_model.parameters():
            param.requires_grad = False

        # Get device from generator model
        self.device = next(self.generator_model.parameters()).device

    def __call__(
        self,
        completions: list[str],
        prompts: Optional[list[str]] = None,
        task_prompt: Optional[list[str]] = None,
        test_list: Optional[list[list[str]]] = None,
        test_imports: Optional[list[list[str]]] = None,
        reference_code: Optional[list[str]] = None,
        **kwargs,
    ) -> list[float]:
        """
        Compute rewards for a batch of compressed encodings.

        This matches the GRPOTrainer reward function signature.

        Args:
            completions: List of compressed encodings generated by Student
            prompts: Formatted prompts (from GRPOTrainer)
            task_prompt: Original MBPP task prompts (NOT passed to Generator)
            test_list: MBPP test assertions (dataset column)
            test_imports: Required imports for tests (dataset column)
            reference_code: Original Python reference solution (for compression ratio)
            **kwargs: Additional arguments from GRPOTrainer

        Returns:
            List of reward floats, one per completion
        """
        batch_size = len(completions)

        # Handle missing columns gracefully
        if task_prompt is None:
            task_prompt = [""] * batch_size
        if test_list is None:
            test_list = [[]] * batch_size
        if test_imports is None:
            test_imports = [[]] * batch_size
        if reference_code is None:
            reference_code = [""] * batch_size

        # Expand all encodings to Python via Generator
        python_codes = self._batch_expand_encodings(encodings=completions)

        # Compute rewards
        rewards = []
        for i in range(batch_size):
            encoding = completions[i]
            python_code = python_codes[i]
            tests = test_list[i] if i < len(test_list) else []
            imports = test_imports[i] if i < len(test_imports) else []
            ref_code = reference_code[i] if i < len(reference_code) else ""

            # Count encoding tokens
            encoding_tokens = len(self.student_tokenizer.encode(encoding))

            # Count reference tokens (for compression ratio)
            if ref_code:
                ref_tokens = len(self.student_tokenizer.encode(ref_code))
            else:
                ref_tokens = encoding_tokens  # Avoid division by zero, ratio 1.0

            # Execute tests with shaped rewards
            passed, partial_reward = self._compute_shaped_reward(
                python_code, tests, imports, encoding_tokens
            )

            # Compute reward
            if passed:
                # Compression ratio reward (BOUNDED to prevent lottery tickets)
                # Ratio > 1.0 means compression (e.g. 50 ref tokens / 25 encoding tokens = 2.0)
                ratio = ref_tokens / max(encoding_tokens, 1)
                ratio_clipped = min(ratio, self.config.max_compression_ratio)

                # Apply length penalty (as originally intended)
                penalty = self.config.length_penalty_coef * encoding_tokens

                reward = (self.config.pass_reward * ratio_clipped) - penalty
            else:
                reward = partial_reward  # Use shaped reward instead of flat fail_reward

            rewards.append(reward)

        return rewards

    def _batch_expand_encodings(
        self,
        encodings: list[str],
    ) -> list[str]:
        """
        Batch expand encodings to Python using the frozen Generator.

        Args:
            encodings: List of compressed encodings from Student

        Returns:
            List of Python code strings
        """
        if not encodings:
            return []

        # Format prompts for generator (encoding only, no task context)
        all_messages = [
            format_generator_prompt(encoding=enc)
            for enc in encodings
        ]

        # Apply chat template
        formatted_prompts = [
            apply_chat_template(self.generator_tokenizer, msgs, add_generation_prompt=True)
            for msgs in all_messages
        ]

        # Tokenize with padding
        inputs = self.generator_tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=1024,
        ).to(self.device)

        # Generate Python code (no gradients needed)
        with torch.no_grad():
            outputs = self.generator_model.generate(
                **inputs,
                max_new_tokens=self.config.generator_max_new_tokens,
                temperature=self.config.generator_temperature,
                do_sample=self.config.generator_temperature > 0,
                pad_token_id=self.generator_tokenizer.pad_token_id,
                eos_token_id=self.generator_tokenizer.eos_token_id,
            )

        # Decode outputs (remove input tokens)
        python_codes = []
        input_length = inputs.input_ids.shape[1]

        for output in outputs:
            generated = output[input_length:]
            python_code = self.generator_tokenizer.decode(
                generated, skip_special_tokens=True
            )
            # Extract code from markdown blocks if present
            python_code = extract_python_code(python_code)
            python_codes.append(python_code)

        return python_codes

    def _execute_tests(
        self,
        python_code: str,
        tests: list[str],
        imports: list[str],
    ) -> bool:
        """
        Execute Python code against MBPP tests in sandbox.

        Args:
            python_code: Generated Python code
            tests: List of assert statements
            imports: List of import statements

        Returns:
            True if all tests pass, False otherwise
        """
        if not tests:
            return False

        test_code = prepare_test_code(tests, imports)
        result = self.sandbox.execute(code=python_code, test_code=test_code)
        return result.success

    def _compute_shaped_reward(
        self,
        python_code: str,
        tests: list[str],
        imports: list[str],
        encoding_tokens: int,
    ) -> tuple[bool, float]:
        """
        Execute tests and compute shaped reward with partial credit.

        This method provides gradual reward signal for early learning by giving
        partial credit for progress toward correct solutions:
        - Syntax validity (+0.2)
        - Code execution without crash (+0.2)
        - Individual test passage (+0.4 proportional)
        - Length bonus for short attempts (+0.2 max)

        Args:
            python_code: Generated Python code to test
            tests: List of test assertions
            imports: List of import statements
            encoding_tokens: Number of tokens in encoding

        Returns:
            Tuple of (passed_all_tests, partial_reward_if_failed)
        """
        # Try full test execution first
        passed = self._execute_tests(python_code, tests, imports)
        if passed:
            return True, 0.0  # Will use compression-based reward

        # Failed - compute partial credit
        partial_reward = self.config.fail_reward  # Start at -1.0

        # 1. Syntax check (+0.2 if valid Python)
        try:
            compile(python_code, '<string>', 'exec')
            partial_reward += 0.2
        except SyntaxError:
            pass

        # 2. Execution check (+0.2 if runs without crash)
        try:
            test_code = prepare_test_code(tests, imports)
            result = self.sandbox.execute(code=python_code, test_code="")
            if result.success:
                partial_reward += 0.2
        except:
            pass

        # 3. Individual test check (+0.4 proportional to tests passed)
        if tests:
            passed_count = 0
            for test in tests:
                test_code = prepare_test_code([test], imports)
                result = self.sandbox.execute(code=python_code, test_code=test_code)
                if result.success:
                    passed_count += 1

            test_fraction = passed_count / len(tests)
            partial_reward += 0.4 * test_fraction

        # 4. Length bonus for short failures (+0.2 max for very short)
        if encoding_tokens < 50:
            length_bonus = 0.2 * (1.0 - encoding_tokens / 50.0)
            partial_reward += length_bonus

        return False, partial_reward

    def expand_single(self, encoding: str) -> str:
        """
        Expand a single encoding to Python (for visualization).

        Args:
            encoding: Compressed encoding to expand

        Returns:
            Expanded Python code
        """
        results = self._batch_expand_encodings([encoding])
        return results[0] if results else ""


def create_reward_function(
    generator_model: PreTrainedModel,
    generator_tokenizer: PreTrainedTokenizer,
    student_tokenizer: PreTrainedTokenizer,
    config: Optional[RewardConfig] = None,
) -> Callable:
    """
    Factory function to create the reward function for GRPOTrainer.

    Args:
        generator_model: Frozen Generator model
        generator_tokenizer: Generator's tokenizer
        student_tokenizer: Student's tokenizer
        config: Reward configuration

    Returns:
        Callable reward function compatible with GRPOTrainer
    """
    reward_fn = DSLRewardFunction(
        generator_model=generator_model,
        generator_tokenizer=generator_tokenizer,
        student_tokenizer=student_tokenizer,
        config=config,
    )
    return reward_fn
