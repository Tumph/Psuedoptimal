"""Custom reward function for LLM-DSL training with GRPOTrainer."""

from dataclasses import dataclass, field
from typing import Callable, Optional

import torch
from transformers import PreTrainedModel, PreTrainedTokenizer

from .prompts import apply_chat_template, extract_python_code, format_generator_prompt
from .sandbox import SafeSandbox, prepare_test_code


@dataclass
class RewardConfig:
    """Configuration for reward computation."""

    pass_reward: float = 10.0
    fail_reward: float = -1.0
    length_penalty_coef: float = 0.05
    max_dsl_tokens: int = 512
    sandbox_timeout: float = 5.0
    sandbox_memory_mb: int = 200
    generator_max_new_tokens: int = 512
    generator_temperature: float = 0.1


class DSLRewardFunction:
    """
    Custom reward function for LLM-DSL GRPO training.

    This class encapsulates the reward computation logic:
    1. Take DSL completions from the Student model
    2. Expand them to Python using the frozen Generator model
    3. Execute Python against MBPP tests in a sandbox
    4. Compute reward: (pass_reward if passed else fail_reward) - (length_penalty * tokens)
    """

    __name__ = "dsl_reward"  # Required by GRPOTrainer

    def __init__(
        self,
        generator_model: PreTrainedModel,
        generator_tokenizer: PreTrainedTokenizer,
        student_tokenizer: PreTrainedTokenizer,
        config: Optional[RewardConfig] = None,
    ):
        """
        Initialize the reward function.

        Args:
            generator_model: Frozen Generator model for DSLâ†’Python expansion
            generator_tokenizer: Generator's tokenizer
            student_tokenizer: Student's tokenizer (for token counting)
            config: Reward configuration
        """
        self.generator_model = generator_model
        self.generator_tokenizer = generator_tokenizer
        self.student_tokenizer = student_tokenizer
        self.config = config or RewardConfig()

        # Initialize sandbox
        self.sandbox = SafeSandbox(
            timeout=self.config.sandbox_timeout,
            memory_limit_mb=self.config.sandbox_memory_mb,
        )

        # Ensure generator is frozen and in eval mode
        self.generator_model.eval()
        for param in self.generator_model.parameters():
            param.requires_grad = False

        # Get device from generator model
        self.device = next(self.generator_model.parameters()).device

    def __call__(
        self,
        completions: list[str],
        prompts: Optional[list[str]] = None,
        task_prompt: Optional[list[str]] = None,
        test_list: Optional[list[list[str]]] = None,
        test_imports: Optional[list[list[str]]] = None,
        **kwargs,
    ) -> list[float]:
        """
        Compute rewards for a batch of DSL completions.

        This matches the GRPOTrainer reward function signature.

        Args:
            completions: List of DSL strings generated by Student
            prompts: Formatted prompts (from GRPOTrainer)
            task_prompt: Original MBPP task prompts (dataset column)
            test_list: MBPP test assertions (dataset column)
            test_imports: Required imports for tests (dataset column)
            **kwargs: Additional arguments from GRPOTrainer

        Returns:
            List of reward floats, one per completion
        """
        batch_size = len(completions)

        # Handle missing columns gracefully
        if task_prompt is None:
            task_prompt = [""] * batch_size
        if test_list is None:
            test_list = [[]] * batch_size
        if test_imports is None:
            test_imports = [[]] * batch_size

        # Expand all DSL completions to Python via Generator
        python_codes = self._batch_expand_dsl(
            dsl_codes=completions,
            task_prompts=task_prompt,
        )

        # Compute rewards
        rewards = []
        for i in range(batch_size):
            dsl_code = completions[i]
            python_code = python_codes[i]
            tests = test_list[i] if i < len(test_list) else []
            imports = test_imports[i] if i < len(test_imports) else []

            # Count DSL tokens for length penalty
            dsl_tokens = len(self.student_tokenizer.encode(dsl_code))

            # Execute tests
            passed = self._execute_tests(python_code, tests, imports)

            # Compute reward
            base_reward = self.config.pass_reward if passed else self.config.fail_reward
            length_penalty = self.config.length_penalty_coef * dsl_tokens
            reward = base_reward - length_penalty

            rewards.append(reward)

        return rewards

    def _batch_expand_dsl(
        self,
        dsl_codes: list[str],
        task_prompts: list[str],
    ) -> list[str]:
        """
        Batch expand DSL codes to Python using the frozen Generator.

        Args:
            dsl_codes: List of DSL strings
            task_prompts: List of original task descriptions

        Returns:
            List of Python code strings
        """
        if not dsl_codes:
            return []

        # Format prompts for generator
        all_messages = [
            format_generator_prompt(task_prompt=tp, dsl_code=dsl)
            for tp, dsl in zip(task_prompts, dsl_codes)
        ]

        # Apply chat template
        formatted_prompts = [
            apply_chat_template(self.generator_tokenizer, msgs, add_generation_prompt=True)
            for msgs in all_messages
        ]

        # Tokenize with padding
        inputs = self.generator_tokenizer(
            formatted_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=1024,
        ).to(self.device)

        # Generate Python code (no gradients needed)
        with torch.no_grad():
            outputs = self.generator_model.generate(
                **inputs,
                max_new_tokens=self.config.generator_max_new_tokens,
                temperature=self.config.generator_temperature,
                do_sample=self.config.generator_temperature > 0,
                pad_token_id=self.generator_tokenizer.pad_token_id,
                eos_token_id=self.generator_tokenizer.eos_token_id,
            )

        # Decode outputs (remove input tokens)
        python_codes = []
        input_length = inputs.input_ids.shape[1]

        for output in outputs:
            generated = output[input_length:]
            python_code = self.generator_tokenizer.decode(
                generated, skip_special_tokens=True
            )
            # Extract code from markdown blocks if present
            python_code = extract_python_code(python_code)
            python_codes.append(python_code)

        return python_codes

    def _execute_tests(
        self,
        python_code: str,
        tests: list[str],
        imports: list[str],
    ) -> bool:
        """
        Execute Python code against MBPP tests in sandbox.

        Args:
            python_code: Generated Python code
            tests: List of assert statements
            imports: List of import statements

        Returns:
            True if all tests pass, False otherwise
        """
        if not tests:
            return False

        test_code = prepare_test_code(tests, imports)
        result = self.sandbox.execute(code=python_code, test_code=test_code)
        return result.success

    def expand_single(self, task_prompt: str, dsl_code: str) -> str:
        """
        Expand a single DSL to Python (for visualization).

        Args:
            task_prompt: Original task description
            dsl_code: DSL to expand

        Returns:
            Expanded Python code
        """
        results = self._batch_expand_dsl([dsl_code], [task_prompt])
        return results[0] if results else ""


def create_reward_function(
    generator_model: PreTrainedModel,
    generator_tokenizer: PreTrainedTokenizer,
    student_tokenizer: PreTrainedTokenizer,
    config: Optional[RewardConfig] = None,
) -> Callable:
    """
    Factory function to create the reward function for GRPOTrainer.

    Args:
        generator_model: Frozen Generator model
        generator_tokenizer: Generator's tokenizer
        student_tokenizer: Student's tokenizer
        config: Reward configuration

    Returns:
        Callable reward function compatible with GRPOTrainer
    """
    reward_fn = DSLRewardFunction(
        generator_model=generator_model,
        generator_tokenizer=generator_tokenizer,
        student_tokenizer=student_tokenizer,
        config=config,
    )
    return reward_fn
